{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcnC1Gikj8LEkZLCdc1t7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeisyData/BIT_IA_Bootcamp/blob/main/S7_C3_pipeline_Inicio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOgJyhJwEd1C"
      },
      "outputs": [],
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Generar un conjunto de datos de ejemplo\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2. Crear diferentes pipelines con distintas técnicas de preprocesamiento"
      ],
      "metadata": {
        "id": "SCYMXcTlEotJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipelines = {\n",
        "    \"Min-Max Scaling\": Pipeline([\n",
        "        (\"scaler\", MinMaxScaler()),\n",
        "        (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "    ]),\n",
        "    \"Standard Scaling\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "    ]),\n",
        "    \"L2 Normalization\": Pipeline([\n",
        "        (\"scaler\", Normalizer(norm=\"l2\")),\n",
        "        (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "hNb8w1OtEgNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3. Evaluar cada pipeline usando validación cruzada"
      ],
      "metadata": {
        "id": "4j49d4eKEusq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = {}\n",
        "for name, pipeline in pipelines.items():\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "    scores[name] = np.mean(cv_scores)\n",
        "    print(f\"{name} - Exactitud promedio (validación cruzada): {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Entrenar y evaluar el mejor modelo en el conjunto de prueba\n",
        "best_pipeline_name = max(scores, key=scores.get)\n",
        "best_pipeline = pipelines[best_pipeline_name]\n",
        "\n",
        "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
        "best_pipeline.fit(X_train, y_train)\n",
        "test_score = best_pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nMejor pipeline: {best_pipeline_name}\")\n",
        "print(f\"Exactitud en el conjunto de prueba: {test_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "jHK_5-EoEnIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}